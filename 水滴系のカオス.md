# 本・水滴系のカオス

## 系の情報とは


過去の分布$P(x)$
↓
↓$P(x'|x)$
↓
未来の分布$P(x')$

- $P(x)$, $P(x')$は確率分布
- $P(x'|x)$は写像

とも言える。

**最小情報分布** $\bar{P}(x)$ とは
- $P(x'|x)$が完全に分かってて、
- でも$P(x)$は全く分からない時に、
- $P(x')$の一番良さげな分布のこと。

一様分布だったりもするし、単振動してることだけが分かってたりしたら端点での分布が多くなる。
系が時間発展で情報を少しでも失っていくのなら、$P(x)$は$\bar{P}(x)$に近づく。

→この時、最初に与えられた条件と$\bar{P}(x)$が独立なことを、**エルゴード性**、と言う。
（調和振動子（単振動のバネ）はいつまでの位相情報を保持するので非エルゴード性）

このとき、$\bar{P}(x)$ = $\bar{P}(x')$であり、こいつは数学的に**不変測度**と言われる。


## 予測可能性

- 知識の量： $I = \int P(x)logP(x)dx$

一様分布では$P(x)=1$だから$I=0$
対数の底が2ならbitsになる。

情報は事前知識との差分的な意味合いもある（相対的な概念）。
推測：$\bar{P}(x)$
実測：$P(x)$とすると、

- **相対情報量**：$I(P,\bar{P})=\int P(x)log(\frac{P}{\bar{P}})dx$

これは実際には離散で測定されるので、
- エントロピー：$H = -\sum p_i log(p_i)$

と定量化できる。
予測不可能性とも考えられる。（p1=1なら、H=0。可能なパターンが等確率で起きる時、Hはmax）
分割数を増やせば増やすほど増加する。

等確率でN分割するのが事前確率だったとすると
\begin{eqnarray}
H' = -\sum_N (1/N)log(1/N) = N*1/N*logN = logN
\end{eqnarray}

もっとちゃんとピークのある分布P(x)のN分割を考える。これのエントロピーをHとして、相対情報量より

$$\begin{eqnarray}
I(P,\bar{P})
&=& \int P(x)log(P/\bar{P})dx
\\&=& \sum P(x)logP - \sum P(x)log\bar{P}
\\&\approx& -H + H'
\\&=& -H + logN
\\
\\∴ H &=& logN - I(P,\bar{P})
\end{eqnarray}
$$

## 9. 写像のエントロピー

相対情報量は、要は分布から分布への変化に関する情報の量ということになる。つまり写像の情報量。
古典力学では、初期状態は一点に定まっていて、そこからの未来も一点に定まっている。（ディラックのデルタ関数からデルタ関数への写像、みたいな）
点はもはやそこで無限に情報量が発散しててワケワカランので、区画にしてやる by コルモゴロフ&シナイ

$x_0→x'$ への写像を考えると
\begin{eqnarray}
H(x'|x_0) = -\sum_j P_{j0} logP_{j0}
\end{eqnarray}
$x→x'$ の写像は、$x_i$が生じる確率P(xi)によるエントロピーの重み付き和なので
\begin{eqnarray}
H(x'|x) &=& -\sum_i P_i \sum_j P_{ji} logP_{ji}
\\&=& -\sum_{ij} P_i P_{ji} logP_{ji}
\end{eqnarray}

分割によって写像エントロピーは色々変わるが、可能な全ての分割の中で最大のエントロピーを与えるものを選べば、そのエントロピーは連続変数$x$だとしてもそれを記述する座標系と独立してる位相不変量となる。byコルモゴロフ、シナイ

$x'=F(x) = 2x$ みたいな写像（ベルヌーイ写像）を考える。

めちゃ決定論っぽく見えるけど、$x$に関する不確定性が$x'$では2倍になっちゃい、つまり、$log2(2)=1bit$の情報量が決定論っぽいのに乗ってしまう。正のエントロピー。
離散で分割を考えても、**エントロピーは状態に対する経路を数え上げて対応する前後の$x'/x$比を考えることでも求められる**。

微視的な変動が巨視的に無視できなくなるのは、微視的に埋め込まれた熱浴が増幅されているもので、これが**カオス**。1対N のNが多いのでしょう。


## 10. ノイズと力学系

分割がノイズよりも小さいと、完全にランダムな写像が分割の分だけ出来るのでエントロピーが発散する。つまり、予測不可能な情報が大量に得られることになる（その分割をまとめれば情報として使えるようになるけど）。つまり、$H(x'|x)$は予測不可能性の指標になる。


## 11. 系に蓄えられる情報量

ある系を見た人Aがその系にちょっと手を加える$x$と未来に見た人Bに情報$x'$を伝えられる。（入力$x$によらず$\bar{P}(x')$が起きる場合は系に情報が蓄積されないので何も伝えられん）

Aさん：$x_0$ 送信
↓ $P(x'|x_0)$・・・これの情報伝達率を求めたい
Bさん：$x'$ 受信

Bさんの驚きは、状態$x_0$を知らない時の極限分布$\bar{P}(x')$に対して相対的に求まる。

初期に$x_0$だった時に、$x'$の全ての情報量の平均は

\begin{eqnarray}
I(x'|x_0) = ∫P(x'|x_0) log\frac{P(x'|x_0)}{\bar{P}(x')} dx'
\end{eqnarray}

初期状態を全て統合すると、メッセージ$P(x)$の平均的な情報伝達量が求まるので
\begin{eqnarray}
I(x'|x) &=& ∫P(x)I(x'|x_0) dx
\\&=& ∫P(x) ∫P(x'|x_0) log\frac{P(x'|x_0)}{\bar{P}(x')} dx' dx
\end{eqnarray}

これを通信路の**情報伝達率**、と言う。

元メッセージ$P(x)$の分布によって伝達率は変わる（良い帯域に沢山情報があれば沢山流れる）。
→maxで伝達出来る時の情報伝達率を**通信容量**、と呼ぶ。


### 力学系だった場合

力学系だと、入力アンサンブルは系自体の性質によって定まる。んで、最小情報分布$\bar{P}(x)$に等しい。それは$\bar{P}(x')$にも等しい。
なので、いつまでも$\bar{P}(x)$のままだから、

系に蓄えられる情報量 $I(x'|x) \equiv ∫\bar{P}(x)∫P(x'|x) log\frac{P(x'|x)}{\bar{P}(x')} dx dx'$・・・過去の状態$x$が分かった時の未来$x'$の予測可能性

これは、$P(x,x') = P(x)P(x'|x)$ を用いると

\begin{eqnarray}
I(x'|x) &=& ∫\bar{P}(x)∫P(x'|x) log \frac{P(x'|x)}{\bar{P}(x')} dx dx'
\\&=&  ∫\bar{P}(x) ∫\frac{P(x,x')}{P(x)} log\frac{ P(x,x')}{P(x) \bar{P}(x')} dx dx'（んで、P(x)=\bar{P}(x)なので）
\\&=& ∫ ∫P(x,x') log \frac{P(x,x')}{\bar{P}(x) \bar{P}(x')}  dx dx'
\end{eqnarray}

ということで、$x$と$x'$入れ替えても同じ式なので、
$I(x'|x) = I(x|x')$・・・つまり、情報伝達率は対称性がある。

しかし、条件付きエントロピーは$H(x)\ne H(x')$なら
$H(x'|x) \ne H(x|x')$
で対称性がない。

ダイナミクス$P(x'|x)$が有限の遷移行列$P_{ij}$で表されるなら、系に蓄えられる情報量は

\begin{eqnarray}
I(x'|x) &=& ∫P(x) ∫P(x'|x_0) log\frac{P(x'|x_0)}{\bar{P}(x')} dx' dx
\end{eqnarray}

を使って、
\begin{eqnarray}
I(x'|x) &=&\sum_{ij}I(x'_j|x_i)
\\&=& \sum_i \bar{P}_i \sum_j P_{ji}log\frac{P_{ji}}{\bar{P}_j}
\\&=& \sum_{ij} \bar{P}_i P_{ji}log\frac{P_{ji}}{\bar{P}_j}
\end{eqnarray}

ここで、
\begin{eqnarray}
I(x'|x) &=&H(x')-H(x'|x)
\end{eqnarray}

つまり、エントロピー（H：ランダム性）の（xを与えられた時とそうでない時の）差が系に蓄積される情報量である。

連続極限でエントロピーは発散するけど、差は発散しない。
系のダイナミクス$P(x'|x)$とそれにより定まる不変分布$\bar{P}(x)$によってこの情報量は決まる。
過去と未来が独立してる場合、$P(x'|x)=P(x')$となり、$I=0$になる。
